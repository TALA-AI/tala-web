{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# RAG 파이프라인 예시 - 엑셀 → Chroma → Retrieval QA\n",
          "\n",
          "이 노트북에서는 엑셀 파일(`.xlsx`)로 관리되는 교통사고 사례 데이터를 로드한 뒤,\n",
          "1. 텍스트 추출 및 청크화\n",
          "2. 임베딩(벡터화)\n",
          "3. Chroma DB에 저장\n",
          "4. 간단한 Retrieval + LLM(QA) 시연\n",
          "을 실행해 봅니다.\n"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "!pip install pandas openpyxl chromadb langchain sentence_transformers\n",
          "# 필요 라이브러리 설치 (최초 1회)\n"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "import os\n",
          "import pandas as pd\n",
          "from langchain.vectorstores import Chroma\n",
          "from langchain.embeddings import HuggingFaceEmbeddings\n",
          "from langchain.docstore.document import Document\n",
          "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
          "from langchain.chains import RetrievalQA\n",
          "from langchain.llms import OpenAI  # 예시, WatsonxLLM 등으로 교체 가능\n",
          "\n",
          "def chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
          "    \"\"\"\n",
          "    RecursiveCharacterTextSplitter를 사용하여 텍스트를 청크로 분할.\n",
          "    \"\"\"\n",
          "    splitter = RecursiveCharacterTextSplitter(\n",
          "        chunk_size=chunk_size,\n",
          "        chunk_overlap=chunk_overlap,\n",
          "    )\n",
          "    return splitter.split_text(text)\n",
          "\n",
          "excel_file = \"./traffic_accident.xlsx\"  # 사용자가 만든 엑셀 파일 경로\n",
          "\n",
          "# 1) 엑셀 로드\n",
          "df = pd.read_excel(excel_file)\n",
          "df.head()"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 2) 텍스트 추출 & 청크화\n",
          "\n",
          "엑셀에 `case_id`, `scenario`, `explanation`, `precedents` 열이 있다고 가정합니다.\n",
          "각 행을 **하나의 문서**로 보고,\n",
          "scenario + explanation + precedents를 합쳐 최종 텍스트로 만든 뒤, chunking 하겠습니다."
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "all_docs = []  # 전체 Document 리스트\n",
          "\n",
          "for idx, row in df.iterrows():\n",
          "    case_id = row.get(\"case_id\", idx)\n",
          "    scenario = str(row.get(\"scenario\", \"\"))\n",
          "    explanation = str(row.get(\"explanation\", \"\"))\n",
          "    precedents = str(row.get(\"precedents\", \"\"))\n",
          "    \n",
          "    # 합친 텍스트\n",
          "    full_text = f\"[사고상황]\\n{scenario}\\n\\n[해설]\\n{explanation}\\n\\n[판례]\\n{precedents}\".strip()\n",
          "    \n",
          "    # 청크화\n",
          "    chunks = chunk_text(full_text, chunk_size=500, chunk_overlap=50)\n",
          "    \n",
          "    for chunk in chunks:\n",
          "        doc = Document(\n",
          "            page_content=chunk,\n",
          "            metadata={\n",
          "                \"case_id\": case_id\n",
          "                # 필요시 scenario, explanation, precedents를 따로 넣어도 됨\n",
          "            }\n",
          "        )\n",
          "        all_docs.append(doc)\n",
          "\n",
          "print(f\"총 문서 청크 개수: {len(all_docs)}\")\n",
          "all_docs[:3]  # 미리보기"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 3) 임베딩 + Chroma 저장\n",
          "\n",
          "저장 후, 나중에 검색 시 바로 로드할 수 있도록 `persist_directory`에 기록하겠습니다."
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "persist_directory = \"chroma_db\"\n",
          "\n",
          "# 임베딩 모델 (HuggingFaceEmbeddings, 기본 all-MiniLM-L6-v2)\n",
          "embeddings = HuggingFaceEmbeddings(\n",
          "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
          ")\n",
          "\n",
          "vectorstore = Chroma.from_documents(\n",
          "    documents=all_docs,\n",
          "    embedding=embeddings,\n",
          "    persist_directory=persist_directory\n",
          ")\n",
          "# DB를 디스크에 저장\n",
          "vectorstore.persist()\n",
          "print(\"Chroma DB 생성 및 저장 완료!\")"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 4) RAG 시연\n",
          "\n",
          "### 4-1) DB 로드 후 Retriever 생성\n",
          "로딩만 하고 싶다면, `Chroma(persist_directory, embedding_function=...)` 로 기존 DB를 불러올 수 있습니다."
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "# 재시작 시, DB 로딩만 할 경우:\n",
          "vectorstore = Chroma(\n",
          "    persist_directory=persist_directory,\n",
          "    embedding_function=embeddings\n",
          ")\n",
          "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
          "\n",
          "# 4-2) LLM 지정(여기서는 OpenAI 예시, WatsonxLLM 등으로 교체 가능)\n",
          "llm = OpenAI(\n",
          "    temperature=0.0,\n",
          "    openai_api_key=os.getenv(\"OPENAI_API_KEY\", \"\")\n",
          ")  # WatsonxLLM으로 대체 가능\n",
          "\n",
          "# RetrievalQA 체인 생성\n",
          "qa_chain = RetrievalQA.from_chain_type(\n",
          "    llm=llm,\n",
          "    chain_type=\"stuff\",\n",
          "    retriever=retriever,\n",
          "    return_source_documents=True\n",
          ")\n",
          "\n",
          "question = \"횡단보도에서 황색신호에 진입한 차량과 보행자 사고 과실비율이 궁금해\"\n",
          "result = qa_chain({\"query\": question})\n",
          "\n",
          "print(\"\\n### User Query:\", question)\n",
          "print(\"\\n### Answer:\", result[\"result\"])\n",
          "print(\"\\n### Source Documents:\")\n",
          "for doc in result[\"source_documents\"]:\n",
          "    print(\"-\", doc.metadata, \"\\n\", doc.page_content[:100], \"...\")"
        ],
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "위에서 `question`을 변경해서 질문해볼 수도 있고,\n",
          "LLM을 WatsonxLLM으로 교체하거나,\n",
          "임베딩 모델을 다르게 교체할 수도 있습니다."
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.9"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 5
  }