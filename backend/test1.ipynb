{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG 파이프라인 예시 - LangChain + Chroma\n",
        "\n",
        "본 노트북에서는 `laws/` 디렉토리에 있는 법령 JSON 파일들을 파싱하여,\n",
        "1) 필요한 텍스트를 추출하고,\n",
        "2) 청크 단위로 나눈 뒤,\n",
        "3) LangChain의 임베딩 모델(여기서는 `HuggingFaceEmbeddings`)로 벡터를 만들고,\n",
        "4) Chroma DB에 저장하는 과정을 시연합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8821ec63",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.9.21' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!pip install \"langchain==0.2.6\"\n",
        "!pip install \"ibm-watsonx-ai==1.0.10\"\n",
        "!pip install \"langchain_ibm==0.1.8\"\n",
        "!pip install \"langchain_community==0.2.6\"\n",
        "!pip install \"sentence-transformers==3.0.1\"\n",
        "!pip install \"chromadb==0.5.3\"\n",
        "!pip install \"pydantic==2.8.2\"\n",
        "!pip install \"langchain-huggingface==0.0.3\"\n",
        "!pip install \"python-dotenv==1.0.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.9.21' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치 (최초 1회)\n",
        "!pip install langchain chromadb sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# 기타 유틸\n",
        "def extract_text_from_law_json(json_data: dict) -> list:\n",
        "    \"\"\"\n",
        "    하나의 법령 JSON에서 필요한 텍스트(문자열)만 추출하여 리스트로 반환.\n",
        "    원하는 필드에 맞춰 자유롭게 수정 가능.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    law = json_data.get(\"법령\", {})\n",
        "\n",
        "    # (1) 개정문\n",
        "    revision_obj = law.get(\"개정문\", {}).get(\"개정문내용\", [])\n",
        "    for paragraph_list in revision_obj:  # 2차원 리스트로 가정\n",
        "        for line in paragraph_list:\n",
        "            if isinstance(line, str) and line.strip():\n",
        "                results.append(line.strip())\n",
        "\n",
        "    # (2) 조문\n",
        "    provisions = law.get(\"조문\", {}).get(\"조문단위\", [])\n",
        "    for provision in provisions:\n",
        "        content = provision.get(\"조문내용\", \"\")\n",
        "        title = provision.get(\"조문제목\", \"\")\n",
        "        if title:\n",
        "            results.append(f\"[조문제목] {title}\")\n",
        "        if content:\n",
        "            results.append(f\"[조문내용] {content}\")\n",
        "        # 항/호 내부 텍스트도 추출\n",
        "        if \"항\" in provision and \"호\" in provision[\"항\"]:\n",
        "            for ho_item in provision[\"항\"][\"호\"]:\n",
        "                ho_no = ho_item.get(\"호번호\")\n",
        "                ho_content = ho_item.get(\"호내용\")\n",
        "                if ho_no:\n",
        "                    results.append(f\"[호번호] {ho_no}\")\n",
        "                if ho_content:\n",
        "                    results.append(f\"[호내용] {ho_content}\")\n",
        "\n",
        "    # (3) 부칙\n",
        "    addenda = law.get(\"부칙\", {}).get(\"부칙단위\", [])\n",
        "    for addendum in addenda:\n",
        "        add_text = addendum.get(\"부칙내용\", [])\n",
        "        for paragraph_list in add_text:\n",
        "            for line in paragraph_list:\n",
        "                if isinstance(line, str) and line.strip():\n",
        "                    results.append(line.strip())\n",
        "\n",
        "    # (4) 제개정이유 (필요하다면)\n",
        "    reasons = law.get(\"제개정이유\", {}).get(\"제개정이유내용\", [])\n",
        "    for paragraph_list in reasons:\n",
        "        for line in paragraph_list:\n",
        "            if isinstance(line, str) and line.strip():\n",
        "                results.append(line.strip())\n",
        "\n",
        "    return results\n",
        "\n",
        "def chunk_text(text_list: list, max_chunk_size: int = 500) -> list:\n",
        "    \"\"\"\n",
        "    길이가 긴 문장을 일정 크기(max_chunk_size)로 분할.\n",
        "    여기서는 단순히 문자열 길이를 기준으로 쪼개는 예시입니다.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    for text in text_list:\n",
        "        # text가 너무 길면 여러 청크로 나눔\n",
        "        if len(text) <= max_chunk_size:\n",
        "            chunks.append(text)\n",
        "        else:\n",
        "            start = 0\n",
        "            while start < len(text):\n",
        "                end = start + max_chunk_size\n",
        "                chunks.append(text[start:end])\n",
        "                start = end\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# \"laws/\" 디렉토리에 있는 모든 json 파일을 읽고, Chroma에 저장\n",
        "\n",
        "# (1) 사용할 임베딩 모델 설정 (LangChain)\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # 예시 모델명\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# (2) JSON 파싱 & 청크 -> 전체 텍스트 리스트 구성\n",
        "laws_dir = \"laws\"  # JSON 파일들이 있는 디렉토리\n",
        "json_files = glob.glob(os.path.join(laws_dir, \"*.json\"))\n",
        "\n",
        "all_docs = []       # 최종적으로 임베딩할 텍스트 목록\n",
        "all_metadatas = []  # 메타데이터 목록 (파일명 등)\n",
        "\n",
        "for file_path in json_files:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        json_data = json.load(f)\n",
        "    \n",
        "    # 텍스트 추출\n",
        "    extracted_texts = extract_text_from_law_json(json_data)\n",
        "    # 청크화\n",
        "    chunks = chunk_text(extracted_texts, max_chunk_size=500)\n",
        "    \n",
        "    # all_docs 에 추가\n",
        "    for chunk in chunks:\n",
        "        all_docs.append(chunk)\n",
        "        all_metadatas.append({\n",
        "            \"source_file\": os.path.basename(file_path),\n",
        "            \"preview\": chunk[:30]  # chunk 앞부분 30자 정도를 메타정보로\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "# (3) Chroma 벡터 스토어 생성\n",
        "# 이미 DB가 있다면 업데이트할 수도 있으나, 여기서는 새로 만듭니다.\n",
        "\n",
        "persist_directory = \"chroma_db\"  # Chroma 데이터가 저장될 폴더\n",
        "\n",
        "# all_docs 와 all_metadatas를 이용해 Chroma 인스턴스 생성\n",
        "vectorstore = Chroma.from_texts(\n",
        "    texts=all_docs,\n",
        "    embedding=embedding_model,\n",
        "    metadatas=all_metadatas,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "# 생성 후, DB를 디스크에 저장\n",
        "vectorstore.persist()\n",
        "\n",
        "print(\"Chroma 벡터DB에 데이터 적재 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위에서 생성한 Chroma DB(`chroma_db` 폴더)는 RAG 파이프라인에서 **검색**용으로 활용할 수 있습니다. \n",
        "예를 들어, 질의 시 LangChain을 통해 아래와 같이 **유사도 검색**이 가능합니다:\n",
        "\n",
        "```python\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# 이미 생성된 DB 불러오기\n",
        "vectorstore = Chroma(\n",
        "    embedding_function=embedding_model, \n",
        "    persist_directory=\"chroma_db\"\n",
        ")\n",
        "\n",
        "# 질의\n",
        "query = \"교통사고처리특례법상 형사합의 관련 내용 알려줘\"\n",
        "docs = vectorstore.similarity_search(query, k=3)\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    print(f\"\\n[{i}] score=?\")\n",
        "    print(doc.metadata)  # 메타데이터(파일명, 미리보기)\n",
        "    print(doc.page_content)  # 실제 텍스트\n",
        "```\n",
        "\n",
        "이렇게 검색된 텍스트들을 LLM에 주입하여 **추론**(답변)을 생성하면, RAG 워크플로우를 구축할 수 있습니다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
